# 📄 Extracting Training Data from Large Language Models (Carlini et al., 2021)

## 🧠 핵심 내용
LLM은 단순히 일반화하는 것이 아니라, **학습 데이터 일부를 원문 그대로 출력할 수 있다.**  
이 논문은 GPT-2 모델을 대상으로, 수백만 문장을 생성해 훈련 데이터를 복원할 수 있음을 보여준다.

---

## 🛠️ 공격 개요: 텍스트 생성 기반 훈련 데이터 추출

### 📌 프롬프트 전략
- “”, “My name is”, “The following license applies to this software:” 등
- Sampling 다양화: Top-k, Top-p, Temperature 조절

### 📌 암기 탐지 방법
1. Perplexity가 낮은 문장 선별
2. 변형(단어 치환, 대소문자 변화 등)에 따른 PPL 비교 (Contrastive PPL)
3. 웹 또는 크롤링 데이터와의 비교 → 완전 일치 문자열만 '암기'로 간주

### 💬 성공적 복원 예시
- MIT License
- 실명 이메일 주소
- UUID (고유 식별자)

---

## 🧪 실험 결과 요약

| 전략 | 추출 수 | 특징 |
|------|--------|------|
| Top-k | 191개 | 일반 샘플링 기반 |
| Internet Seed | 273개 | 프롬프트 다양화 |
| zlib 압축 기반 | 67% 정확도 | 엔트로피 기반 탐지 |
| GPT-2 XL | 긴 문장도 암기 | 33회 등장한 문장도 복원 |
| GPT-2 Small | 성능 낮음 | 359회 본 것도 암기 실패 |

---

## ⚠️ 시사점 및 위험 요소

- 단 몇 번 등장한 데이터도 암기됨 → 희귀할수록 위험
- 큰 모델일수록 더 정확하고 정교하게 암기
- "오버피팅되지 않으면 안전"이라는 통념은 잘못됨
- 암기 방지는 일부 대응만 가능 (DP, 정규화 등)

---

## 🔐 대응 전략 비교

| 전략 | 설명 | 장점 | 한계 |
|------|------|------|------|
| Differential Privacy | 학습 중 노이즈 추가 | 이론적 보호 | 성능 저하, 확장 어려움 |
| 데이터 정제 | 민감 정보 사전 제거 | 쉬움 | 완벽 방지는 어려움 |
| Fine-tuning 조절 | 다운스트림에서 조정 | 제한적 효과 | 재암기 가능 |
| 모델 감사 | 사람이 검토 | 강력한 검증 | 비용 & 시간 소요 |

---

## 📸 실험 이미지 예시 (추후 삽입)
> 이곳에 sampling 결과 예시 이미지 삽입 예정

---

## 🧪 실습 예정: GPT-2로 샘플링 후 PPL 기반 분석

Colab 기반 구현 예정:  
- GPT-2 토큰화 & 샘플링  
- 생성 문장 → Perplexity 계산  
- 변형 문장들과 PPL 비교 (contrastive perplexity)

➡ 추후 extract_sample.ipynb 업로드 예정

---

✍️ *by sunhangai — 논문 리뷰 프로젝트 ②*
